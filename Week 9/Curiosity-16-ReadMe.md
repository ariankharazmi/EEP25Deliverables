# Curiosity-16.2025.08.07 README.MD - Author: Arian Kharazmi - License: Apache 2.0

## Model Summary
Parameters: 404M Parameters  
Base: GPT-2 Medium (Decoder)  
Tokenizer: AutoTokenizer  
Training: 2-Phase Full SFT  
Purpose: Research Model -- Proof of Concept  
Strengths: Short factual responses, small stories, basic reasoning  
Limitations: Hard-limit at 1-2 Sentences, tends to misunderstand, no safety filter, prone to hallucinate

## Description
Curiosity-16 is a small research model (based on pre-trained GPT-2 Medium) that has 404M Parameters. It uses training samples from 11 diverse HuggingFace datasets.

## Phase 1 Datasets used: (Generalization)
* oasst1
* fineweb-bbc-news
* wikipedia-20250620
* eli5
* squad_v2
* dolly-15k-instruction-alpaca-format

## Phase 2 Datasets used: (Task-focused)
* agieval_lsat_ar
* agieval_lsat_lr
* agieval_lsat_rc
* gsm8k
* alpaca-cleaned

## Inference System Prompt
system_prompt = (
“You are a concise factual assistant named Curiosity-16. “
“Rules: “
“• Answer in 1–2 plain sentences. No lists unless asked. “
“• Do not write emails, salutations, or ask the user questions back. “
“• If unsure, say “I’m not sure.” “
“• If asked for illegal or harmful instructions, refuse briefly and suggest a safe alternative.\n\n”)

## Capabilities & Limitations
Curiosity-16 excels at 1-2 sentence generation, We observe that text generation beyond 1-2 may result in reduced factual accuracy, so a hard limit has been added to keep responses direct and focused.

Curiosity-16 is great for general purpose questions such as "Who is ____"?, "Write a short story about ___.", "Where should I visit in ____?"

Curiosity-16 does NOT feature guardrails as this is purely a research model not meant for daily casual use. Responses generated by the model do not reflect the opinions of the author.

Do not rely on Curiosity-16 for factual decisions without verification.

Curiosity-16 serves as culmination of the Summer 2025 EEP Research co-op's findings and deliverables.

## Citation
@software{Kharazmi_Curiosity-16-2025,
  author = {Kharazmi, Arian},
  title = {Curiosity-16: A Small Research Model (GPT-2 Medium Fine-tune)},
  year = {2025},
  url = {https://huggingface.co/ariankharazmi/Curiosity-16}
}

## Acknowledgements
Thank you to the University of Cincinnati for allowing me to work on AI-focused Research, this has been an incredible opportunity for me. Thank you to my Co-op Advisor, Kenny Pope, and my EEP Mentor, Josh Anness, for their continued support and guidance. Thanks to the Hugging Face community and the maintainers of the datasets listed above.